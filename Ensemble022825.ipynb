{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDW8ooL3qW4IHm5QLwXygx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osommersell264/MLsessions/blob/main/Ensemble022825.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuj_l0Vv2LMX"
      },
      "outputs": [],
      "source": [
        "# Create an Ensemble\n",
        "\n",
        "# Step 1: Install and Load Libraries\n",
        "!pip install ucimlrepo scikit-learn pandas numpy matplotlib seaborn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Fetch Dataset\n",
        "predict_students_dropout_and_academic_success = fetch_ucirepo(id=697)\n",
        "\n",
        "# Data (as pandas dataframes)\n",
        "X = predict_students_dropout_and_academic_success.data.features\n",
        "y = predict_students_dropout_and_academic_success.data.targets\n",
        "\n",
        "# Drop enrolled\n",
        "X = X[y != 'Enrolled']\n",
        "y = y[y != 'Enrolled']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Calculate Base-Rate Accuracy (Naive Model)\n",
        "base_rate_accuracy = y.value_counts().max() / len(y)\n",
        "print(f\"Base-Rate Accuracy (Naive Model): {base_rate_accuracy:.2f}\")\n",
        "\n",
        "# Create a few classifiers classifiers\n",
        "modelA = DecisionTreeClassifier(max_depth=5, random_state=42)  # Simple decision tree\n",
        "modelB = LogisticRegression(max_iter=1000, random_state=42)    # Logistic regression for binary classification\n",
        "modelC = RandomForestClassifier(n_estimators=100, random_state=42)  # More complex decision trees combined\n",
        "\n",
        "# Step 4: Data Cleaning & Preprocessing\n",
        "# Convert target variable to binary format: Graduate = 1, Dropout = 0\n",
        "y = y.iloc[:,0].map({\"Graduate\": 1, \"Dropout\": 0}) # Apply map to the target column\n",
        "\n",
        "# Encode categorical variables to numerical values\n",
        "for col in X.select_dtypes(include=['object']).columns:\n",
        "    X[col] = LabelEncoder().fit_transform(X[col])\n",
        "\n",
        "# Standardize numerical features to improve model performance\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 5: Train-Test Split\n",
        "# We split the dataset into 80% training and 20% testing to evaluate model performance.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Create the Ensemble Model\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('decision_tree', modelA),\n",
        "        ('logistic_regression', modelB),\n",
        "        ('random_forest', modelC)\n",
        "    ],\n",
        "    voting='soft'  # Changed to soft voting for better probability distribution\n",
        ")\n",
        "\n",
        "# Step 7: Train the Ensemble Model\n",
        "ensemble.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make Predictions\n",
        "y_pred_ensemble = ensemble.predict(X_test)\n",
        "\n",
        "# Step 9: Evaluate the Ensemble Model\n",
        "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
        "print(f\"Ensemble Model Accuracy: {ensemble_accuracy:.2f}\")\n",
        "print(\"Ensemble Classification Report:\\n\", classification_report(y_test, y_pred_ensemble))\n",
        "\n",
        "# Step 10: Feature Importance Analysis (Random Forest)\n",
        "feature_importances = pd.Series(modelC.feature_importances_, index=X.columns)\n",
        "print(\"Feature Importances (Random Forest):\\n\", feature_importances.sort_values(ascending=False))\n",
        "\n",
        "# Step 11: Compare with Individual Models\n",
        "modelA.fit(X_train, y_train)\n",
        "y_pred_dt = modelA.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "print(f\"Decision Tree Accuracy: {dt_accuracy:.2f}\")\n",
        "\n",
        "modelB.fit(X_train, y_train)\n",
        "y_pred_lr = modelB.predict(X_test)\n",
        "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy:.2f}\")\n",
        "\n",
        "modelC.fit(X_train, y_train)\n",
        "y_pred_rf = modelC.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy:.2f}\")\n",
        "\n",
        "# Step 12: Visualizing Confusion Matrices\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_ensemble), annot=True, fmt='d', cmap='Reds', ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Ensemble Model')\n",
        "\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_dt), annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Decision Tree')\n",
        "\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, fmt='d', cmap='Greens', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Logistic Regression')\n",
        "\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Oranges', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Random Forest')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Step 11: Analyzing False Positives & False Negatives\n",
        "print(\"Confusion Matrix - Ensemble Model:\\n\", confusion_matrix(y_test, y_pred_ensemble))\n",
        "print(\"Confusion Matrix - Decision Tree:\\n\", confusion_matrix(y_test, y_pred_dt))\n",
        "print(\"Confusion Matrix - Logistic Regression:\\n\", confusion_matrix(y_test, y_pred_lr))\n",
        "print(\"Confusion Matrix - Random Forest:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
        "\n",
        "# Step 12: Answer Key Questions\n",
        "# Q1: How does the ensemble balance False Positives/Negatives compared to individual models?\n",
        "# Answer: The ensemble averages predictions, reducing variance compared to a single model.\n",
        "\n",
        "# Q2: What actions will happen based on predictions?\n",
        "# Answer: Schools could use these predictions to identify at-risk students and intervene early.\n",
        "\n",
        "# Q3: What are the data requirements?\n",
        "# Answer: We need historical student records, attendance, GPA, and demographic factors to train the model.\n",
        "\n",
        "# Q4: What classification problem does this solve?\n",
        "# Answer: This is a binary classification problem (Graduate = 1, Dropout = 0).\n",
        "\n",
        "\"\"\"\n",
        "Answer Key Questions\n",
        "Q1: How does the ensemble balance False Positives/Negatives compared to individual models?\n",
        "Answer:\n",
        "\n",
        "An ensemble model combines multiple classifiers, reducing the weaknesses of individual models.\n",
        "If one model is too sensitive (high false positives) and another is too strict (high false negatives), the ensemble balances their predictions through majority voting.\n",
        "Example: If the Decision Tree predicts \"Dropout,\" but both Logistic Regression and Random Forest predict \"Graduate,\" the ensemble will classify the student as \"Graduate\" (majority vote).\n",
        "This reduces variance and increases reliability over a single model.\n",
        "Q2: What actions will happen based on predictions?\n",
        "Answer:\n",
        "\n",
        "Schools and policymakers can identify at-risk students early and provide interventions such as tutoring, mentoring, or financial aid.\n",
        "Example: If a student is predicted to drop out, the school can assign counselors or offer academic support to improve retention.\n",
        "This model can help allocate resources efficiently and improve graduation rates.\n",
        "Q3: What are the data requirements?\n",
        "Answer:\n",
        "To train this model, we need historical and demographic student data, including:\n",
        "âœ… Academic Records: GPA, test scores, course completion rates.\n",
        "âœ… Attendance Data: Absenteeism, tardiness.\n",
        "âœ… Demographic Information: Socioeconomic status, family background.\n",
        "âœ… Behavioral Data: Disciplinary records, engagement in extracurricular activities.\n",
        "\n",
        "ðŸ”¹ More data â†’ Better model performance!\n",
        "\n",
        "Q4: What classification problem does this solve?\n",
        "Answer:\n",
        "\n",
        "This is a binary classification problem where the goal is to predict student graduation status:\n",
        "Graduate (1)\n",
        "Dropout (0)\n",
        "The model helps categorize students based on historical trends and their likelihood of success.\n",
        "Why binary classification?\n",
        "The target variable has only two possible outcomes (Graduate or Dropout).\n",
        "The model learns from past patterns and predicts the probability of each studentâ€™s outcome.\n",
        "ðŸš€ Key Takeaways\n",
        "âœ” Ensemble models improve accuracy and reduce false positives/negatives.\n",
        "âœ” Schools can use predictions for targeted interventions to support at-risk students.\n",
        "âœ” High-quality, diverse data improves model performance.\n",
        "âœ” This is a binary classification task that predicts student outcomes (Graduate vs. Dropout).\n",
        "\n",
        "\"\"\""
      ]
    }
  ]
}